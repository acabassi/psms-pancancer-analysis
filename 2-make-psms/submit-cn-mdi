#!/bin/bash

## Section 1: SLURM Commands
## Full documentation can be found here: https://slurm.schedmd.com/sbatch.html

## A short name for the job, to be shown in SLURM output
#SBATCH -J cnmdi

## Wall-clock time limit for for your jobs. Use format HH:MM:SS
## Maximum value 36:00:00.
#SBATCH --time=02:00:00

## Number of cores per node. For single-core jobs, this number should be '1'. 
## The maximum value is 32.
#SBATCH --cpus-per-task=1

## The system can send emails when your job starts and stops.
## Values include BEGIN, END, ALL, and TIME_LIMIT_80 and TIME_LIMIT_90 
## (reaching 80% or 90% of time limit.) Specify ARRAY_TASKS to receive
## a separate mail for each task. Multiple values can be given, separated by a comma.
#SBATCH --mail-type=BEGIN,END,FAIL

## The project account name.
#SBATCH -A mrc-bsu-sl2-cpu

## The partition. Use skylake for normal jobs (6GB/core) or skylake-himem (12GB/core)
#SBATCH -p skylake

## Array jobs:
## SBATCH --array=1-5

##  - - - - - - - - - - - - - -

## Section 2: Modules

# All scripts should include the first three lines.

. /etc/profile.d/modules.sh                # Leave this line (enables the module command)
module purge                               # Removes all modules still loaded
module load rhel7/default-peta4            # REQUIRED - loads the basic environment

# Load the latest R version.
# Before running your code, you should run R and install any required packages.
module load r-3.6.1-gcc-5.4.0-zrytncq

# Load gcc/8 to use mclust
module load gcc/8

# Load gdal and sf to use PReMiuM
module load gdal-2.3.1-gcc-5.4.0-m7j7nw6
module load geos-3.6.2-gcc-5.4.0-vejexvy

#! Insert additional module load commands after this line if needed:

## - - - - - - - - - - -

## Section 3: Run your application

# You can use any arbitrary set of Linux commands here

CMD="Rscript psm-CN-mdi.R $@"

###############################################################
### You should not have to change anything below this line ####
###############################################################

JOBID=$SLURM_JOB_ID

echo -e "JobID: $JOBID\n======"
echo "Time: `date`"
if [ $SLURM_JOB_NUM_NODES -gt 1 ]; then
        echo "Running on nodes: $SLURM_JOB_NODELIST"
else
        echo "Running on node: `hostname`"
fi

echo "Current directory: `pwd`"
echo -e "\nNum tasks = $SLURM_CPUS_PER_TASK, Num nodes = $SLURM_JOB_NUM_NODES, OMP_NUM_THREADS = $OMP_NUM_THREADS"
echo -e "\nExecuting command:\n==================\n$CMD\n"

eval $CMD
